{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Actor and Critic network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=300, fc2_units=150):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \"\"\" initialization \"\"\"\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=300, fc2_units=150):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        \"\"\" initialization \"\"\"\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Replay buffer implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        #self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploration noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent logic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DDPG_agent():\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"init the agent\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random_seed\n",
    "        \n",
    "        # Construct Actor networks\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Construct Critic networks \n",
    "        self.critic_local = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # noise processing\n",
    "        self.noise = OUNoise((num_agents,action_size), random_seed)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        #self.memory = MemoryBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # convert state from numpy to pytorch array \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        # use actor_local to predict action\n",
    "        # turn nn into evaluation mode \n",
    "        self.actor_local.eval()\n",
    "        # turn off computing gradients\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        # turn the nn into training mode\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        \n",
    "        # clipping the action from min to max\n",
    "        return np.clip(action, -1, 1)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for i in range(state.shape[0]):\n",
    "            self.memory.add(state[i, :], action[i], reward[i], next_state[i, :], done[i])\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" reset noise \"\"\"\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        # mean squared error loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        # zero_grad because we do not want to accumulate \n",
    "        # gradients from other batches, so needs to be cleared\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        # compute derivatives for all variables that\n",
    "        # requires_grad-True\n",
    "        critic_loss.backward()\n",
    "        # update those variables that requires_grad-True\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        # take the current states and predict actions\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # -1 * (maximize) Q value for the current prediction\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        # zero_grad because we do not want to accumulate \n",
    "        # gradients from other batches, so needs to be cleared\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # compute derivatives for all variables that\n",
    "        # requires_grad-True\n",
    "        actor_loss.backward()\n",
    "        # update those variables that requires_grad-True\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize the Unity env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install a few packages.  This line will take a few minutes to run!\n",
    "!pip -q install ./python\n",
    "!pip install unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher_Windows_x86_64_20/Reacher.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = DDPG_agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)\n",
    "n_episodes = 1000\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def ddpg(n_episodes=2000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agents.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        #print (state.shape)\n",
    "        for t in range(max_t):\n",
    "            action = agents.act(state)\n",
    "            #print (action)\n",
    "            #time.sleep(100)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            #print (state.shape)\n",
    "            #print (rewards.shape)\n",
    "            agents.step(state, action, rewards, next_state, dones)\n",
    "            state = next_state\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                print('\\tSteps: ', t)\n",
    "                break \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.3f}'.format(i_episode, \n",
    "                                                                          np.mean(scores_deque), \n",
    "                                                                          np.mean(score)))\n",
    "        average_score = np.mean(scores_deque)\n",
    "        if i_episode % print_every == 20 or average_score > 30:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth') \n",
    "            if average_score > 30:\n",
    "                break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.62\tScore: 0.625\n",
      "Episode 2\tAverage Score: 0.90\tScore: 1.167\n",
      "Episode 3\tAverage Score: 0.89\tScore: 0.886\n",
      "Episode 4\tAverage Score: 0.89\tScore: 0.894\n",
      "Episode 5\tAverage Score: 0.87\tScore: 0.761\n",
      "Episode 6\tAverage Score: 0.92\tScore: 1.164\n",
      "Episode 7\tAverage Score: 0.98\tScore: 1.339\n",
      "Episode 8\tAverage Score: 1.02\tScore: 1.292\n",
      "Episode 9\tAverage Score: 1.06\tScore: 1.367\n",
      "Episode 10\tAverage Score: 1.10\tScore: 1.490\n",
      "Episode 11\tAverage Score: 1.18\tScore: 2.017\n",
      "Episode 12\tAverage Score: 1.29\tScore: 2.467\n",
      "Episode 13\tAverage Score: 1.42\tScore: 2.940\n",
      "Episode 14\tAverage Score: 1.55\tScore: 3.340\n",
      "Episode 15\tAverage Score: 1.67\tScore: 3.372\n",
      "Episode 16\tAverage Score: 1.79\tScore: 3.470\n",
      "Episode 17\tAverage Score: 1.91\tScore: 3.900\n",
      "Episode 18\tAverage Score: 2.06\tScore: 4.584\n",
      "Episode 19\tAverage Score: 2.27\tScore: 6.093\n",
      "Episode 20\tAverage Score: 2.50\tScore: 6.770\n",
      "Episode 21\tAverage Score: 2.66\tScore: 6.016\n",
      "Episode 22\tAverage Score: 2.86\tScore: 6.917\n",
      "Episode 23\tAverage Score: 3.08\tScore: 8.070\n",
      "Episode 24\tAverage Score: 3.40\tScore: 10.563\n",
      "Episode 25\tAverage Score: 3.81\tScore: 13.690\n",
      "Episode 26\tAverage Score: 4.17\tScore: 13.334\n",
      "Episode 27\tAverage Score: 4.62\tScore: 16.175\n",
      "Episode 28\tAverage Score: 5.06\tScore: 17.075\n",
      "Episode 29\tAverage Score: 5.54\tScore: 18.879\n",
      "Episode 30\tAverage Score: 6.04\tScore: 20.584\n",
      "Episode 31\tAverage Score: 6.62\tScore: 24.021\n",
      "Episode 32\tAverage Score: 7.27\tScore: 27.440\n",
      "Episode 33\tAverage Score: 7.94\tScore: 29.433\n",
      "Episode 34\tAverage Score: 8.67\tScore: 32.571\n",
      "Episode 35\tAverage Score: 9.42\tScore: 35.103\n",
      "Episode 36\tAverage Score: 10.16\tScore: 35.915\n",
      "Episode 37\tAverage Score: 10.87\tScore: 36.615\n",
      "Episode 38\tAverage Score: 11.55\tScore: 36.448\n",
      "Episode 39\tAverage Score: 12.20\tScore: 36.908\n",
      "Episode 40\tAverage Score: 12.82\tScore: 37.280\n",
      "Episode 41\tAverage Score: 13.43\tScore: 37.595\n",
      "Episode 42\tAverage Score: 14.01\tScore: 37.838\n",
      "Episode 43\tAverage Score: 14.56\tScore: 37.758\n",
      "Episode 44\tAverage Score: 15.09\tScore: 37.907\n",
      "Episode 45\tAverage Score: 15.60\tScore: 38.073\n",
      "Episode 46\tAverage Score: 16.09\tScore: 37.828\n",
      "Episode 47\tAverage Score: 16.54\tScore: 37.562\n",
      "Episode 48\tAverage Score: 16.98\tScore: 37.292\n",
      "Episode 49\tAverage Score: 17.39\tScore: 37.322\n",
      "Episode 50\tAverage Score: 17.80\tScore: 37.635\n",
      "Episode 51\tAverage Score: 18.18\tScore: 37.508\n",
      "Episode 52\tAverage Score: 18.56\tScore: 37.554\n",
      "Episode 53\tAverage Score: 18.91\tScore: 37.409\n",
      "Episode 54\tAverage Score: 19.27\tScore: 38.162\n",
      "Episode 55\tAverage Score: 19.59\tScore: 37.204\n",
      "Episode 56\tAverage Score: 19.91\tScore: 37.152\n",
      "Episode 57\tAverage Score: 20.21\tScore: 37.342\n",
      "Episode 58\tAverage Score: 20.50\tScore: 36.591\n",
      "Episode 59\tAverage Score: 20.77\tScore: 36.871\n",
      "Episode 60\tAverage Score: 21.05\tScore: 37.318\n",
      "Episode 61\tAverage Score: 21.32\tScore: 37.481\n",
      "Episode 62\tAverage Score: 21.58\tScore: 37.550\n",
      "Episode 63\tAverage Score: 21.83\tScore: 37.524\n",
      "Episode 64\tAverage Score: 22.08\tScore: 37.345\n",
      "Episode 65\tAverage Score: 22.31\tScore: 37.210\n",
      "Episode 66\tAverage Score: 22.53\tScore: 36.723\n",
      "Episode 67\tAverage Score: 22.75\tScore: 37.727\n",
      "Episode 68\tAverage Score: 22.98\tScore: 37.915\n",
      "Episode 69\tAverage Score: 23.18\tScore: 37.314\n",
      "Episode 70\tAverage Score: 23.39\tScore: 37.657\n",
      "Episode 71\tAverage Score: 23.59\tScore: 37.188\n",
      "Episode 72\tAverage Score: 23.78\tScore: 37.296\n",
      "Episode 73\tAverage Score: 23.97\tScore: 37.644\n",
      "Episode 74\tAverage Score: 24.15\tScore: 37.968\n",
      "Episode 75\tAverage Score: 24.33\tScore: 37.170\n",
      "Episode 76\tAverage Score: 24.49\tScore: 36.939\n",
      "Episode 77\tAverage Score: 24.67\tScore: 38.099\n",
      "Episode 78\tAverage Score: 24.84\tScore: 37.627\n",
      "Episode 79\tAverage Score: 24.99\tScore: 37.295\n",
      "Episode 80\tAverage Score: 25.15\tScore: 37.777\n",
      "Episode 81\tAverage Score: 25.31\tScore: 37.674\n",
      "Episode 82\tAverage Score: 25.46\tScore: 37.315\n",
      "Episode 83\tAverage Score: 25.59\tScore: 36.925\n",
      "Episode 84\tAverage Score: 25.73\tScore: 36.662\n",
      "Episode 85\tAverage Score: 25.86\tScore: 37.247\n",
      "Episode 86\tAverage Score: 25.99\tScore: 37.021\n",
      "Episode 87\tAverage Score: 26.12\tScore: 37.536\n",
      "Episode 88\tAverage Score: 26.25\tScore: 37.044\n",
      "Episode 89\tAverage Score: 26.37\tScore: 37.501\n",
      "Episode 90\tAverage Score: 26.51\tScore: 38.301\n",
      "Episode 91\tAverage Score: 26.63\tScore: 37.556\n",
      "Episode 92\tAverage Score: 26.75\tScore: 37.430\n",
      "Episode 93\tAverage Score: 26.86\tScore: 37.386\n",
      "Episode 94\tAverage Score: 26.97\tScore: 37.402\n",
      "Episode 95\tAverage Score: 27.08\tScore: 37.217\n",
      "Episode 96\tAverage Score: 27.19\tScore: 37.773\n",
      "Episode 97\tAverage Score: 27.30\tScore: 37.608\n",
      "Episode 98\tAverage Score: 27.40\tScore: 37.457\n",
      "Episode 99\tAverage Score: 27.50\tScore: 37.562\n",
      "Episode 100\tAverage Score: 27.61\tScore: 37.678\n",
      "Episode 101\tAverage Score: 27.97\tScore: 37.441\n",
      "Episode 102\tAverage Score: 28.33\tScore: 36.990\n",
      "Episode 103\tAverage Score: 28.70\tScore: 37.397\n",
      "Episode 104\tAverage Score: 29.06\tScore: 37.096\n",
      "Episode 105\tAverage Score: 29.42\tScore: 36.890\n",
      "Episode 106\tAverage Score: 29.78\tScore: 37.369\n",
      "Episode 107\tAverage Score: 30.15\tScore: 37.505\n",
      "Episode 107\tAverage Score: 30.15\n"
     ]
    }
   ],
   "source": [
    "score = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl8VPW9//HXJzskYU2AQNh3EAWMiLt1qUtbl9pNbbXWltpN6+3tfm3t797e28Vq/VlsL/e6YGuVVqv401ZBBKlVUTaRAIGwBghZyL4v8/n9MQNFCBAgk8nMvJ+PRx6ZOTmT8zkcMu853+/3fI+5OyIiEr8SIl2AiIhEloJARCTOKQhEROKcgkBEJM4pCERE4pyCQEQkzikIRETiXNiDwMwSzWyNmb0Yej7azFaY2RYzW2BmKeGuQUREjq47zgjuAjYe8vznwAPuPh6oBG7vhhpEROQoLJxXFptZLjAf+CnwL8DHgDJgiLu3mdk5wL3ufsWxfk9WVpaPGjUqbHWKiMSiVatWlbt79vHWSwpzHb8GvgNkhp4PBKrcvS30fDcwrKMXmtkcYA7AiBEjWLlyZZhLFRGJLWa2szPrha1pyMw+CpS6+6pDF3ewaoenJO4+z93z3D0vO/u4gSYiIicpnGcE5wHXmNnVQBrQh+AZQj8zSwqdFeQCe8NYg4iIHEfYzgjc/fvunuvuo4DPAK+5+83AUuATodVuBRaGqwYRETm+SFxH8F3gX8yskGCfwSMRqEFERELC3VkMgLsvA5aFHm8DZnXHdkVE5Ph0ZbGISJxTEIiIxDkFgYjEHHdn4do9FFc3RrqUqKAgkJhT3djKim37aWptD+t2Vu+q5F/+tJb65rbjrxzDGlvaeWldMW3tgUiXctBfVu/hrqfXcvvjK8P+/yAWdEtnsUhXag294SQnBj/HuDtby+p5fXMZSzaW8M72CtoCTlZGCrecM4qbzx7BwIzULq2hsr6Fr/5hNftqmhjWrxff+vDELv39XWFLSS1vb6/guulDyUxLPqnfsbWsjoeWbOHa6cP40KRBHa7z6yWb+e/Xt3HhhGzm3jTjpLfVVfZWNXLvC/mMGtibDcU1/Oxvm7j3mqkRramnUxBI1CiubmT+mzv544qd1Le0M2JAb4YP6E1hSS17q5sAGD8ogy9eMIapQ/vwl9W7uX/xZuYuLeTGWSO446KxDOmbdsp1uDvfeXYd++ubmTVqAPOWb+PTZw0nt3/vU/7dXVHb42/uYMG7RWzaVwvAX9cV89htZ5GWnHhwnfqWdjJSj/7nX9fcxkNLtvDoP7bT2u68tqmURXdfdMS/X3VjK0++vYsJgzP4R2E5n/zdWzz6+bMY2q/XwXXaA87cpYVsLK5hck4fpuT0YfbYgcfc/skKBJxvP/Me7e488YWzmf/WDh55Yzvnjh3Ih6cOOeHf9etXN1NW18wlkwZz/rgseqUkHnX96oZW9lY3MnFwJgkJHU2i0HOFddK5rpKXl+eaayh+ldY0cd+iAv6yeg8Bd648bQijs9LZUd7Azop6hvfvzQXjs7lgfBbDB3zwzbiwtJbfvb6N59fsIcGMT+Tl8pWLxh6xnrvzwnt7uW9RAW3tTt6oAcwc0Y+kBKO8roWaplYm5/Th/HFZLNlYwj0L87nno1O46rQhXPKrZVw2eTC/uWnmKe1na3uA/L017KpoYHdlA/vrWuidkkjvlCSG9kvj4gmD6Nv72J+2X8nfx5d/v4rpw/tx/YxhJCQY9zy/niunDmHuzTPZVlbHD557n3W7q/nVp87go6cPPeJ3LMrfx48W5rOvpolP5eXyqbzhfO6Rd5g1egCP33YWZv98k/vNa1u4b9FmXrrzfPbXtfDVJ1eTlpzIPR+dzDVnDKW5LcA3n17Ly/n7GNavF3urG3GHwX1S+el107hsyuCT+rdav6eakpomLp38wdfPf3MHP34hn/+8fho3nT2C5rZ2bvjtmxRVNPLtKyaSN6o/EwYd/43a3fnRwnx+//ZO0pITaGoNkJqUwDljB3Lh+GzOHTeQqoZW1u+pJn9vDe8VVbGtvB6AsdnpfP7cUXx8Zi7ppxB2JTVNvL1tPxdNyKZf75Obrd/MVrl73nHXUxBIT9Xc1s6jb+zgN69toaU9wM1nj+T280cf8SbeGUUVDfzu9a38eeVuAu7cMDOXa6cPpamtnerGVp54aydrdlUxdWgfRmels2pnJcWhswzg4JsBgBlcNCGbxz4ffFN8YPFmHlyyhWfuOIe8UQMOvqauuY0lG0sYOTCd6cP7dVhXIOC8+H4xi/L38frmMmqb/tnf0Cs5kaa2dg78iSYlGOeNy+KyKYM5Z8wAxmZnfOBNua09wId/vRwDXvnmhSSFms4eeWM7//7iBs4c2Z91u6tIT00it38v1u+p4dtXTOSrF4/FHbaV1/GrRZv52/p9TBqSyX9+fBozR/QH4Pdv7eCehfn89PrTuPnskUCwb+D8n7/GtNy+PH5b8NKggn21/Ouf3+P9PdWcM2YgzW3trCmq4odXT+aLF4yhvrmNNbuq+I+XNrBpXy3XTh/KvR+bSv/0zr/RLd1Uyh1/WBUMmcvGc9el4zEzfv/WDu79fxu4YHzWwWMDsL28nlseXUFRRbDjuHdKIoMyUxmQnkJGWjJt7QGa2wL0SUviiqlDuGLqEB5csoXH39zBly8aw7cun8g72yt4dWMJyzeXHXzDP2Bwn1ROz+3H9OH96Nc7mQXvFrFudzUD0lP4zU0zOHdsFgAtbQH+75It7K9v4c5Lx5HTtxeHq29u45evFPD65jK2h7bz25tnctW0nE7/+xxKQSBRbd3uKr71p/fYUlrH5VMG88OrJzMqK/2Uf29xdSO/W7aVp94toqXtn52b2ZmpfPuKidwwM5fE0KfFkpomzGBA7xQSE4yCklre2FJOYWkd/3rFRLJC/Q4NLW1cct/rJCYYs0YPYFBmKrsrG3l1YwnNbQEy05L4210XHNF0VNfcxr8sWMuiDSVkZaRyyaRsLpowiHGDMsjt34v01CTcnabWAAUltfxtfTF/e38fuyoaAMjKSOGzs0dy5yXjSUgwnlyxkx8+t555nzvziGaQX7y8iYeXbeXjM4fxw6snk5GWxHeeWcfCtXuZODiTvVWN1Da3kZKUwF2XjmfOhWMO9sFA8BPyLY++w8odlTx880wumpDN79/eyY9fyGfBnNmcPWbgwXXbA84f39nFL1/eRFNbgF9/ejpXH/ZG1tIW4OFlhfzmtUIGZqRw/6emc964rCOO15MrdvLgq1u4eloOn509go3Ftdy9YC2TcjKZMCiTv6zZw42zRpCcaDzx1k4umTSIBz8z/Yh+CnenqKKRlTsreH9PNfvrWqiob6G2qZWUpARSkxLZXdnAjv0NmIE73H7+aP7tI5M/ELYQ/FDxzvYKBmakMHVoX7IzU4/Y1updlXz32ffZUV7Pj6+ZysUTsvn6U2t4r6iKpAQjMcG4/fzRfPmisfTtFay1tKaJL8x/l43FtVw8IZvZYwYye8xApgztc/D/5IlSEEhUam0P8NBrhcxdWkh2Rir/dcM0PjSx407KU1Fa08Tmkjoy0pLICH1CPtCGfjL+UVjOrxYVUFLTTFldM5mpSXzk9BzOHTuQf/3zOiYOyWTBnNkHP6Xv3F/Pl55Yydayen5w9WRuO3dUp9qV3Z2d+xtYsX0/izeU8urGEq6fMYwff2wKl92/nNFZvfnTl8854s0LoLyu+WB4HfhdDy/byrKCUiYOyWTasL6cNy7rqH0dxdWN3PDwm+ytbmJ0Vjp1zW2MGNCbZ+7oeHuV9S3UNrUxYuDRz+DW76nmrqfXsK28njkXjOHuyyccPA7Pr9nD3X9ay+isdIoqGmhtd8wgb2R/Hvn8WWSmJnHfogLmLt0KwJwLx/DdKyed9Jumu5O/t4aX3i8mMy2Jr1w0tsP96qyaplbuemoNSwvKSE1KICUpgV/ccDqnDevLfYsKWLh2L2nJCVw5dQgfmjSIX7xcQGVDC3NvmnnUjvkTpSCQqFNZ38Idf1jFiu0VfHzmMH78sakHPy1FkwN/UwfeRBauDQ5lvPOSccy5aCyPvbGdeX/fRoIZc2+ayfnjj/wk3NntPLxsK798pYCsjFTK65p59ivncubI/l22L4drbmvn5fX7mP/mDlbvquLx287i4lMM6saWdv79pQ38ccUusjJSuP38MQzr34u7F6xl1qgBPHbbWdQ1t7Hg3SJKapr4/lWTP9Bpu3DtHpISEvjI6SfXfBJO7QHngcWbeW93FT+9btoHQjF/bzV/XLGLF97bS21TG9mZqTz2+bM4bVjfLtu+gkB6tIr6Fu5fXMDknD5cNnkwtU1t3D7/XYqrm/jFDadz3YwO71cUtb795/d4ZvVu+vZKpqqhlcsmD+Kej05h5MBTb+56ZtVuvvfsOj48dTAP33xmF1TbOdWNrV0a1O/uqOCh1wpZvrkMgDNy+/Lkl2aHZXRRT9LU2s7b2/YzJacPg/qc+qi2QykIpMdqDzi3PvoObxSWH1yWkpRAn7Qk/vtzeWH9RBsp9c1tfPaRFfRJS+buyycctfP4ZBVVNJCdmXpKzVs9xbrdVSzeUMIXzht9Qp3IcqTOBkFsR630SPcvLuCNwnJ+9vFpzBzZn8UbSti1v4FvXDquR4zFD4f01CSe++p5Yfv9JzOSqqc6Pbcfp+d2bVDKsSkIpFstyt/H3KVb+cxZw/nMrBEATBiceZxXiUg4aa4h6Ta7Kxv41p/e4/TcvrrkX6QHURBItwgEnO88s46AO3NvmhkTbdkisSJsQWBmaWb2jpm9Z2b5ZvaT0PLHzWy7ma0NfU0PVw3Sczy5Yidvbt3PDz8yJabas0ViQTj7CJqBS9y9zsySgTfM7G+hn33b3Z8J47alB9m5v57//OsmLhifxY2zhke6HBE5TNiCwIPjUutCT5NDXz1/rKp0ue89+z5JCcbPbzj9lK7UFJHwCGsfgZklmtlaoBRY7O4rQj/6qZmtM7MHzKxrJ4qXHmV/XTNvbdvPly8a84GpiUWk5whrELh7u7tPB3KBWWZ2GvB9YBJwFjAA+G5HrzWzOWa20sxWlpWVhbNMCaP3dlcBcNYhs3KKSM/SLaOG3L0KWAZc6e7FHtQMPAbMOspr5rl7nrvnZWdnd0eZEgZrdlWRmGBMy+26+VNEpGuFc9RQtpn1Cz3uBVwGbDKznNAyA64D1oerBom8tUVVTBycSe8UXbso0lOF868zB5hvZokEA+dP7v6imb1mZtmAAWuBO8JYg0RQIOCsLariY2cceRcsEek5wjlqaB0wo4Pll4Rrm9KzbCuvo7aprcsnWBORrqUriyVs1uwKdhTPHKEgEOnJFAQSNmuLqshMS2JMVkakSxGRY1AQSNis2VXFGbn9OnULRhGJHAWBhEVjSzsFJbXMULOQSI+nIJCweH9PNe0BV0exSBRQEEhYrNlVCaAgEIkCCgIJi7VFVQwf0IuBGZpKSqSnUxBIWKzbXc0Zuu+sSFRQEEiXq21qZU9VI5Nz+kS6FBHpBAWBdLnNJcHbUEzUTelFooKCQLpcwb5aACYOURCIRAMFgXS5zSW1pKckMkw3ohGJCgoC6XKb9tUwYUimrigWiRIKAulS7k7Bvlr1D4hEEQWBdKmyumYqG1rVPyASRRQE0qXUUSwSfRQE0qUOBoGahkSiRjjvWZxmZu+Y2Xtmlm9mPwktH21mK8xsi5ktMLOUcNUg3a9gXy1ZGamaWkIkioTzjKAZuMTdzwCmA1ea2Wzg58AD7j4eqARuD2MN0s0KSmqZpGYhkagStiDwoLrQ0+TQlwOXAM+Els8HrgtXDdK92gPO5pJa9Q+IRJmw9hGYWaKZrQVKgcXAVqDK3dtCq+wGhoWzBuk+RRUNNLUG1D8gEmXCGgTu3u7u04FcYBYwuaPVOnqtmc0xs5VmtrKsrCycZUoX2aQRQyJRqVtGDbl7FbAMmA30M7Ok0I9ygb1Hec08d89z97zs7OzuKFNO0eaSWsxg/GDdrF4kmoRz1FC2mfULPe4FXAZsBJYCnwitdiuwMFw1SPfaWFzDiAG96Z2SdPyVRaTHCOdfbA4w38wSCQbOn9z9RTPbADxtZv8BrAEeCWMN0k3aA87b2/ZzyaTBkS5FRE5Q2ILA3dcBMzpYvo1gf4HEkPy91VQ2tHLhhKxIlyIiJ0hXFkuX+PuWcgDOG6cgEIk2CgLpEss3lzF1aB+ydEWxSNRREMgpq2tuY9XOSi6coNFdItFIQSCn7O2t+2kLOBeMV7OQSDRSEMgp+/uWMnolJ3LmyP6RLkVEToKCQE7Z8i3lnDN2IKlJiZEuRUROgoJATklRRQPby+vVLCQSxRQEckoODBu9YLw6ikWilYJATsmqnZVkZaQyNjs90qWIyElSEMgpKSyrY+KQDMws0qWIyElSEMhJc3e2ltYxLluzjYpEMwWBnLSSmmbqmtsYN0hBIBLNFARy0gpLg3ciHasgEIlqCgI5aYWlwTuSqWlIJLopCOSkFZbVkZmWRHamJpoTiWYKAjlphaV1jBukEUMi0U5BICetsLRezUIiMSCc9ywebmZLzWyjmeWb2V2h5fea2R4zWxv6ujpcNUj4VDe0Ul7XrBFDIjEgnPcsbgO+5e6rzSwTWGVmi0M/e8Dd7wvjtiXMCstCHcUKApGoF857FhcDxaHHtWa2ERgWru1J9zowdFRBIBL9uqWPwMxGEbyR/YrQoq+b2Toze9TMNIl9FCosrSMlKYHc/r0jXYqInKKwB4GZZQDPAt909xrgt8BYYDrBM4ZfHeV1c8xspZmtLCsrC3eZcoIKS+sYk5VOYoJGDIlEu7AGgZklEwyBJ939LwDuXuLu7e4eAP4HmNXRa919nrvnuXtedramOO5pCsvq1CwkEiPCOWrIgEeAje5+/yHLcw5Z7XpgfbhqkPBobGlnd2WjgkAkRoRz1NB5wOeA981sbWjZD4AbzWw64MAO4MthrEHCYGtZHe7qKBaJFeEcNfQG0FED8l/DtU3pHlvLNGJIJJboymI5YVtL60gwGDVQdyUTiQUKAjlhhWV1jBjQm7TkxEiXIiJdQEEgJ+zAZHMiEhsUBHJC2toD7ChvYKwmmxOJGQoCOSFFlY20tAd0VzKRGKIgkBOiOYZEYo+CQE7IwfsUq2lIJGYoCOSEbC2rIzszlb69kiNdioh0EQWBnJDC0jrdlUwkxigIpNPcna0aOioScxQE0mmltc3UNrcpCERijIJAOm2rOopFYlKng8DMzjez20KPs81sdPjKkp6oUJPNicSkTgWBmf0Y+C7w/dCiZOAP4SpKeqbC0joyUpMY3Cc10qWISBfq7BnB9cA1QD2Au+8FMsNVlPRMW8vqGDsog+A9h0QkVnQ2CFrc3QneTAYz0/zDcaiwtI6x2Tr0IrGms0HwJzP7b6CfmX0JeJXg/YYlTtQ0tVJS06z+AZEY1Kk7lLn7fWZ2OVADTAR+5O6Lw1qZ9CgHRgzpYjKR2HPcIDCzROAVd78M6PSbv5kNB54AhgABYJ67P2hmA4AFwCiC9yz+lLtXnnjp0p1W7ggeoilD+0S4EhHpasdtGnL3dqDBzPqe4O9uA77l7pOB2cDXzGwK8D1gibuPB5aEnksP93L+PqYO7UNu/96RLkVEulhnb17fBLxvZosJjRwCcPc7j/YCdy8GikOPa81sIzAMuBa4OLTafGAZwaGp0kOV1jSxamcl37p8QqRLEZEw6GwQvBT6OilmNgqYAawABodCAncvNrNBR3nNHGAOwIgRI05209IFXtlQAsCVpw2JcCUiEg6d7Syeb2YpwIGPhAXu3tqZ15pZBvAs8E13r+nsGHR3nwfMA8jLy/NOvUjC4uX1xYzJTteIIZEY1dkriy8GtgBzgYeBzWZ2YSdel0wwBJ5097+EFpeYWU7o5zlA6UnULd2ksr6Ft7dVcNVpQ3QhmUiM6ux1BL8CPuzuF7n7hcAVwAPHeoEF3zUeATa6+/2H/OgF4NbQ41uBhSdWsnSnVzeW0B5wrpyaE+lSRCRMOttHkOzuBQeeuPvm0Kf9YzkP+BzBTua1oWU/AH5G8AK124FdwCdPsGbpRq/k72NYv16cNkzDRkViVWeDYKWZPQL8PvT8ZmDVsV7g7m8AR2tLuLST25UIqmtuY/mWcj579kg1C4nEsM4GwVeArwF3EnxzX06wr0Bi2PLNZbS0Bbhi6uBIlyIiYdTZIEgCHjzQ1h+62lhzEce4VzeU0K93MmeO7B/pUkQkjDrbWbwE6HXI814EJ56TGNXWHmBpQSmXTBxEUqJuZCcSyzr7F57m7nUHnoQea66BGLZ6VxWVDa1cOlnNQiKxrrNBUG9mMw88MbM8oDE8JUlP8OrGEpITjQsnZEW6FBEJs872EXwT+LOZ7SV4c5qhwKfDVpVE3KsbS5g9ZiCZaccbJSwi0e6YZwRmdpaZDXH3d4FJBKePbgNeBrZ3Q30SAVvL6thWVs9lahYSiQvHaxr6b6Al9PgcgheEzQUqCc0DJLFnycbgJHOXTu5wPkARiTHHaxpKdPeK0ONPE7y5zLPAs4dcLSwx5tWNpUwakql7D4jEieOdESSa2YGwuBR47ZCfdbZ/QaLItrI6Vu6o4PIpahYSiRfHezN/CnjdzMoJjhL6O4CZjQOqw1ybdDN350cL80lPSeJz54yMdDki0k2OGQTu/lMzWwLkAIvc/cB9ARKAb4S7OOleL71fzBuF5fzkmqkMykyLdDki0k2O27zj7m93sGxzeMqRSKlrbuPfX9zA1KF9+OxsnQ2IxBO18wsAv168mdLaZn732TNJTNBMoyLxRJPICG3tAf6wYifXTx/GjBGaYE4k3igIhK1l9TS1Bjh/vKaTEIlHCgIhf29wANhpw/pGuBIRiYSwBYGZPWpmpWa2/pBl95rZHjNbG/q6Olzbl87L31tDalICY7LSI12KiERAOM8IHgeu7GD5A+4+PfT11zBuXzopf281k3L66L4DInEqbH/57r4cqDjuihJR7k7+3hpOG6qb04vEq0h8BPy6ma0LNR0ddYiKmc0xs5VmtrKsrKw764srRRWN1Da1MXWo+gdE4lV3B8FvgbHAdKAY+NXRVnT3ee6e5+552dnZ3VVf3DnQUTxVZwQicatbg8DdS9y93d0DwP8As7pz+3Kk9XurSUwwJg7JjHQpIhIh3RoEZpZzyNPrgfVHW1e6R/7eGsYPyiAtOTHSpYhIhIRtigkzewq4GMgys93Aj4GLzWw6wdtd7gC+HK7tS+fk763hAl1IJhLXwhYE7n5jB4sfCdf25MSV1jRRVtvMaeooFolrGjgeZ4qrGw92EOfvrQHUUSwS7zT7aJz50cJ8Fm8o4cZZw+mTlgzAFAWBSFxTEMSZDXtrGNInjQXvFhFwGDWwN5mhQBCR+KSmoThS09TKnqpGbjl3JM9/7TymD+/HlaflHP+FIhLTdEYQRzbvqwVg4uBMTs/tx/NfOy/CFYlIT6Azgjiy6UAQ6OIxETmEgiCOFOyrJTM1iWH9ekW6FBHpQRQEcaRgXy0ThmRipnsSi8g/KQjihLtTUFKrZiEROYKCIE6U1DRT3djKJAWBiBxGQRAnNu0LXkU8cbCCQEQ+SEEQJwo0YkhEjkJBECcK9tUyuE8q/XqnRLoUEelhFARxYtO+WiYO0ZxCInIkBUEcaGsPUFhWp45iEemQgiAO7NjfQEtbQB3FItIhBUEcUEexiBxL2ILAzB41s1IzW3/IsgFmttjMtoS+9w/X9uWfNu2rIcFg3KCMSJciIj1QOM8IHgeuPGzZ94Al7j4eWBJ6LmG2tqiKCYMzdYN6EelQ2ILA3ZcDFYctvhaYH3o8H7guXNuXoEDAWVtUxYwROvkSkY51dx/BYHcvBgh9H9TN248728rrqG1qY8aIfpEuRUR6qB7bWWxmc8xspZmtLCsri3Q5UWv1rioAZuqMQESOoruDoMTMcgBC30uPtqK7z3P3PHfPy87O7rYCY82aXVX0SUtiTFZ6pEsRkR6qu4PgBeDW0ONbgYXdvP24s2ZXJdNH9CchQfcgEJGOhXP46FPAW8BEM9ttZrcDPwMuN7MtwOWh5xImdc1tbC6pZcZw9Q+IyNGF7eb17n7jUX50abi2KR+0rqiKgKOOYhE5ph7bWSynbk1RsKN4xnB1FIvI0SkIYtiaXZWMzU6nb+/kSJciIj2YgiBGuTtrdulCMhE5PgVBjCqqaGR/fYv6B0TkuBQEMWrF9v2A+gdE5PgUBDHI3Zn/1g5GZ6Vr6mkROS4FQQxavqWc9XtquOOiMSTqQjIROQ4FQQx6eGkhOX3TuH5GbqRLEZEooCCIMat2VrBiewVfvGAMKUk6vCJyfHqniDEPL91K/97J3DhreKRLEZEooSCIUu0BP2LZ+j3VLNlUym3njaZ3SthmDxGRGKMgiEKFpbXM+umr/PrVzQeXBQLOjxauZ2B6CreeOypyxYlI1FEQRJnqxla+9MQq9te38OCSLbxZWA7AM6t2s3pXFd+7ahJ9e2lKCRHpPAVBFGkPOHc9vYaiigbmf2EWo7PS+eaCtWwtq+O//raRs0b154aZGikkIidGQRBF7l9cwLKCMn5y7VQumpDNQzfOoKqhlWseeoOapjb+/brTdAMaETlhCoIoUVnfwrzl2/j4jGHcfPZIAKYO7cv3r55EfUs7t507iklD+kS4ShGJRhpaEiVefL+Y1nbn9gtGf2D5588dxZScPpplVEROWkSCwMx2ALVAO9Dm7nmRqCOaPLd6NxMHZzIl54Of+s2Ms8cMjFBVIhILInlG8CF3L4/g9qPGjvJ6Vu+q4rtXTsJMfQAi0rXURxAFnluzBzO4bsbQSJciIjEoUkHgwCIzW2VmcyJUQ1Rwd55fu4dzxgwkp2+vSJcjIjEoUkFwnrvPBK4CvmZmFx6+gpnNMbOVZrayrKys+yvsIVbvqmTn/gaunzEs0qWISIyKSBC4+97Q91LgOWBWB+vMc/c8d8/Lzs7u7hJ7jGdW7SYtOYGrpuVEuhQRiVHdHgRmlm5mmQceAx8G1nd3HdFgWUEpT79bxPUzcslI1UhfEQmPSLy7DAaeC41+SQL+6O4vR6COHm3n/nrufGoNEwdncs9HJ0f9N/N2AAAKJ0lEQVS6HBGJYd0eBO6+DTiju7cbTeqb25jzxCoSEoz/uSVPU0qLSFhp+GgPs7+umS/OX8mW0loeunEGwwf0jnRJIhLj9FGzB3mvqIqv/GEV5fUt/PITZ3DB+PjtJBeR7qMgiIDqxlYq6luobWqlvK6Zgn11bNpXw9/W7yM7I5Vn7ziXabl9I12miMQJBUE3am0PcN+iAuYt34YfdqfJYf168ZFpOdzz0SkMSE+JTIEiEpcUBN1k1/4GvvH0Gt4rquKTZ+ZyztiBZKYlMyA9mXGDMnVXMRGJGAVBN1i+uYyvPbkaDObeNJOPnK6Lw0Sk51AQhNnT7+zih8+vZ/ygDP7nljyNAhKRHkdBECaBgHPfogIeXraVCydkM/emGWSmqflHRHoeBUEY1DW3cfeCtSzeUMKNs0bwf66dSnKiLtkQkZ5JQdDFiioa+OL8lRSW1XHvx6Zw67mjdDMZEenRFARdaGNxDZ97ZAWt7c7822Zx/visSJckInJcCoIusm53Fbc8+g5pSYks+PLZjM3OiHRJIiKdoiA4RdWNrfyjsJzvPrOOvr2T+eMXZzNioEYGiUj0UBAcQ11zG8sKSlmzq4o+aclkZaaQnJhAUUUDO/c3sGlfDVtK63CH0VnpPPnFsxnaT7eTFJHoEjdB8NQ7u9hT2cgXzh993CkctpfX859/3cjrm8toaQuQkpRAS1vg4M8TDIb178W47Aw+dvpQZo7sz5kj+5OWnBju3RAR6XJxEQS/XbaVn7+8CYDH/rGdL5w/mg9NGkQg4LQHnNFZ6QzqkwbAc2t282/PrScpMYHPnj2SK6YOJm/UANoDTmVDC82tAXL6pWk4qIjEjJgPgrlLC/nlKwVcO30oX7l4LA8tKeSh14JfhxqdlU5O3zTe3LqfWaMG8OCN08np+89mnsQEY3AoLEREYklMB8GBELh+xjDu++QZJCYYc2+eyd2ldRRVNpAYGt9fsK+WFdsr2Fhcw52XjOPOS8eTpE/8IhInIhIEZnYl8CCQCPyvu/8sHNsZnZXOJ8/M5Wc3nE5iwj8v6ho3KINxg/45vPPCCdl86cIx4ShBRKTH6/YgMLNEYC5wObAbeNfMXnD3DV29raun5XD1NM30KSJyLJFo/5gFFLr7NndvAZ4Gro1AHSIiQmSCYBhQdMjz3aFlH2Bmc8xspZmtLCsr67biRETiTSSCoKMZ2PyIBe7z3D3P3fOys3UTdxGRcIlEEOwGhh/yPBfYG4E6RESEyATBu8B4MxttZinAZ4AXIlCHiIgQgVFD7t5mZl8HXiE4fPRRd8/v7jpERCQoItcRuPtfgb9GYtsiIvJBunxWRCTOmfsRA3Z6HDMrA3aewEuygPIwldOTxMN+xsM+Qnzsp/ax+4109+MOu4yKIDhRZrbS3fMiXUe4xcN+xsM+Qnzsp/ax51LTkIhInFMQiIjEuVgNgnmRLqCbxMN+xsM+Qnzsp/axh4rJPgIREem8WD0jEBGRToq5IDCzK82swMwKzex7ka6nK5jZcDNbamYbzSzfzO4KLR9gZovNbEvoe/9I13qqzCzRzNaY2Yuh56PNbEVoHxeEpiWJambWz8yeMbNNoWN6TqwdSzO7O/R/db2ZPWVmabFwLM3sUTMrNbP1hyzr8NhZ0P8NvRetM7OZkav82GIqCA656c1VwBTgRjObEtmqukQb8C13nwzMBr4W2q/vAUvcfTywJPQ82t0FbDzk+c+BB0L7WAncHpGqutaDwMvuPgk4g+D+xsyxNLNhwJ1AnrufRnAqmc8QG8fyceDKw5Yd7dhdBYwPfc0BfttNNZ6wmAoCYvSmN+5e7O6rQ49rCb5xDCO4b/NDq80HrotMhV3DzHKBjwD/G3puwCXAM6FVYmEf+wAXAo8AuHuLu1cRY8eS4PQ1vcwsCegNFBMDx9LdlwMVhy0+2rG7FnjCg94G+plZj7xlYqwFQaduehPNzGwUMANYAQx292IIhgUwKHKVdYlfA98BAqHnA4Eqd28LPY+F4zkGKAMeCzWB/a+ZpRNDx9Ld9wD3AbsIBkA1sIrYO5YHHO3YRc37UawFQaduehOtzCwDeBb4prvXRLqermRmHwVK3X3VoYs7WDXaj2cSMBP4rbvPAOqJ4magjoTayK8FRgNDgXSCzSSHi/ZjeTxR8/831oIgZm96Y2bJBEPgSXf/S2hxyYFTzdD30kjV1wXOA64xsx0Em/QuIXiG0C/UvACxcTx3A7vdfUXo+TMEgyGWjuVlwHZ3L3P3VuAvwLnE3rE84GjHLmrej2ItCGLypjehtvJHgI3ufv8hP3oBuDX0+FZgYXfX1lXc/fvunuvuowget9fc/WZgKfCJ0GpRvY8A7r4PKDKziaFFlwIbiKFjSbBJaLaZ9Q793z2wjzF1LA9xtGP3AnBLaPTQbKD6QBNSj+PuMfUFXA1sBrYCP4x0PV20T+cTPKVcB6wNfV1NsA19CbAl9H1ApGvtov29GHgx9HgM8A5QCPwZSI10fV2wf9OBlaHj+TzQP9aOJfATYBOwHvg9kBoLxxJ4imC/RyvBT/y3H+3YEWwamht6L3qf4CiqiO9DR1+6slhEJM7FWtOQiIicIAWBiEicUxCIiMQ5BYGISJxTEIiIxDkFgcQ0M2s3s7WHfB3zKl4zu8PMbumC7e4ws6yTeN0VZnavmfU3s7+eah0inZF0/FVEolqju0/v7Mru/rtwFtMJFxC88OpC4B8RrkXihIJA4lJoKosFwIdCi25y90Izuxeoc/f7zOxO4A6C04BvcPfPmNkA4FGCF0c1AHPcfZ2ZDSR4sVE2wYum7JBtfZbgtMwpBCcL/Kq7tx9Wz6eB74d+77XAYKDGzM5292vC8W8gcoCahiTW9TqsaejTh/ysxt1nAb8hOK/R4b4HzHD30wkGAgSvmF0TWvYD4InQ8h8Db3hwIrkXgBEAZjYZ+DRwXujMpB24+fANufsCgnMOrXf3aQSvyJ2hEJDuoDMCiXXHahp66pDvD3Tw83XAk2b2PMGpICA43ccNAO7+mpkNNLO+BJtyPh5a/pKZVYbWvxQ4E3g3OO0OvTj6hHLjCU5HANDbg/eeEAk7BYHEMz/K4wM+QvAN/hrgHjObyrGnFu7odxgw392/f6xCzGwlkAUkmdkGIMfM1gLfcPe/H3s3RE6NmoYknn36kO9vHfoDM0sAhrv7UoI3y+kHZADLCTXtmNnFQLkH7w1x6PKrCE4kB8FJyD5hZoNCPxtgZiMPL8Td84CXCPYP/ILghInTFQLSHXRGILGuV+iT9QEvu/uBIaSpZraC4AeiGw97XSLwh1CzjxG8125VqDP5MTNbR7Cz+MD0wz8BnjKz1cDrBKdixt03mNm/AYtC4dIKfA3Y2UGtMwl2Kn8VuL+Dn4uEhWYflbgUGjWU5+7lka5FJNLUNCQiEud0RiAiEud0RiAiEucUBCIicU5BICIS5xQEIiJxTkEgIhLnFAQiInHu/wPwWCse7DmHTwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaff19b780>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(score)+1), score)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Watch a smart agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rum021\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:995: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    }
   ],
   "source": [
    "agents.actor_local.load_state_dict(torch.load('checkpoint_actor.pth', map_location='cpu'))\n",
    "agents.critic_local.load_state_dict(torch.load('checkpoint_critic.pth', map_location='cpu'))\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]        \n",
    "states = env_info.vector_observations                  \n",
    "scores = np.zeros(num_agents)                          \n",
    "\n",
    "for i in range(200):\n",
    "    actions = agents.act(states, add_noise=False)                    \n",
    "    env_info = env.step(actions)[brain_name]        \n",
    "    next_states = env_info.vector_observations        \n",
    "    rewards = env_info.rewards                        \n",
    "    dones = env_info.local_done                 \n",
    "    scores += rewards                         \n",
    "    states = next_states                              \n",
    "    if np.any(dones):                              \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
