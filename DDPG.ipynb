{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        \"\"\" initialization \"\"\"\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=400, fc2_units=300):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "        \"\"\" initialization \"\"\"\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        #self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "\"\"\" Original Code by @jaara: https://github.com/jaara/AI-blog/blob/master/SumTree.py\n",
    "\"\"\"\n",
    "\n",
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = numpy.zeros( 2*capacity - 1 )\n",
    "        self.data = numpy.zeros( capacity, dtype=object )\n",
    "\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "    \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class MemoryBuffer(object):\n",
    "    \"\"\" Memory Buffer Helper class for Experience Replay\n",
    "    using a double-ended queue or a Sum Tree (for PER)\n",
    "    \"\"\"\n",
    "    def __init__(self, buffer_size, batch_size, with_per = False):\n",
    "        \"\"\" Initialization\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        if(with_per):\n",
    "            # Prioritized Experience Replay\n",
    "            self.alpha = 0.5\n",
    "            self.epsilon = 0.01\n",
    "            self.buffer = SumTree(buffer_size)\n",
    "        else:\n",
    "            # Standard Buffer\n",
    "            self.buffer = deque()\n",
    "        self.count = 0\n",
    "        self.with_per = with_per\n",
    "        self.buffer_size = buffer_size\n",
    "        \n",
    "    def add(self, state, action, reward, new_state, done, error=None):\n",
    "        \"\"\" Save an experience to memory, optionally with its TD-Error\n",
    "        \"\"\"\n",
    "\n",
    "        experience = (state, action, reward, done, new_state)\n",
    "        if(self.with_per):\n",
    "            priority = self.priority(error[0])\n",
    "            self.buffer.add(priority, experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            # Check if buffer is already full\n",
    "            if self.count < self.buffer_size:\n",
    "                self.buffer.append(experience)\n",
    "                self.count += 1\n",
    "            else:\n",
    "                self.buffer.popleft()\n",
    "                self.buffer.append(experience)\n",
    "\n",
    "    def priority(self, error):\n",
    "        \"\"\" Compute an experience priority, as per Schaul et al.\n",
    "        \"\"\"\n",
    "        return (error + self.epsilon) ** self.alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return self.count\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\" Current Buffer Occupation\n",
    "        \"\"\"\n",
    "        return self.count\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        \"\"\" Sample a batch, optionally with (PER)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "\n",
    "        # Sample using prorities\n",
    "        if(self.with_per):\n",
    "            T = self.buffer.total() // self.batch_size\n",
    "            for i in range(self.batch_size):\n",
    "                a, b = T * i, T * (i + 1)\n",
    "                s = random.uniform(a, b)\n",
    "                idx, error, data = self.buffer.get(s)\n",
    "                batch.append((*data, idx))\n",
    "            idx = np.array([i[5] for i in batch])\n",
    "        # Sample randomly from Buffer\n",
    "        elif self.count < self.batch_size:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            idx = None\n",
    "            batch = random.sample(self.buffer, self.batch_size)\n",
    "\n",
    "        # Return a batch of experience\n",
    "        s_batch = torch.from_numpy(np.array([i[0] for i in batch])).float().to(device)\n",
    "        a_batch = torch.from_numpy(np.array([i[1] for i in batch])).float().to(device)\n",
    "        r_batch = torch.from_numpy(np.array([i[2] for i in batch])).float().to(device)\n",
    "        d_batch = torch.from_numpy(np.array([i[3] for i in batch], dtype='int')).float().to(device)\n",
    "        new_s_batch = torch.from_numpy(np.array([i[4] for i in batch])).float().to(device)\n",
    "        \n",
    "        #states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        #actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        #rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        #next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        #dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        \n",
    "        return s_batch, a_batch, r_batch, new_s_batch, d_batch #, idx\n",
    "\n",
    "    def update(self, idx, new_error):\n",
    "        \"\"\" Update priority for idx (PER)\n",
    "        \"\"\"\n",
    "        self.buffer.update(idx, self.priority(new_error))\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\" Clear buffer / Sum Tree\n",
    "        \"\"\"\n",
    "        if(self.with_per): self.buffer = SumTree(buffer_size)\n",
    "        else: self.buffer = deque()\n",
    "        self.count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        #print (x.shape)\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class DDPG_agent():\n",
    "    def __init__(self, state_size, action_size, num_agents, random_seed):\n",
    "        \"\"\"init the agent\"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random_seed\n",
    "        \n",
    "        # Construct Actor networks\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "        \n",
    "        # Construct Critic networks \n",
    "        self.critic_local = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "        \n",
    "        # noise processing\n",
    "        self.noise = OUNoise((num_agents,action_size), random_seed)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        #self.memory = MemoryBuffer(BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "    def act(self, state, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        # convert state from numpy to pytorch array \n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        # use actor_local to predict action\n",
    "        # turn nn into evaluation mode \n",
    "        self.actor_local.eval()\n",
    "        # turn off computing gradients\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        # turn the nn into training mode\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "        \n",
    "        # clipping the action from min to max\n",
    "        return np.clip(action, -1, 1)\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for i in range(state.shape[0]):\n",
    "            self.memory.add(state[i, :], action[i], reward[i], next_state[i, :], done[i])\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\" reset noise \"\"\"\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        # mean squared error loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        # zero_grad because we do not want to accumulate \n",
    "        # gradients from other batches, so needs to be cleared\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        # compute derivatives for all variables that\n",
    "        # requires_grad-True\n",
    "        critic_loss.backward()\n",
    "        # update those variables that requires_grad-True\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        # take the current states and predict actions\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # -1 * (maximize) Q value for the current prediction\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        # zero_grad because we do not want to accumulate \n",
    "        # gradients from other batches, so needs to be cleared\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # compute derivatives for all variables that\n",
    "        # requires_grad-True\n",
    "        actor_loss.backward()\n",
    "        # update those variables that requires_grad-True\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the env Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport gym\\nimport random\\nfrom collections import deque\\nimport matplotlib.pyplot as plt\\n%matplotlib inline\\nenv = gym.make(\\'Pendulum-v0\\')\\nenv.seed(2)\\nagent = DDPG_agent(state_size=3, action_size=1, random_seed=2)\\n\\ndef ddpg(n_episodes=1000, max_t=300, print_every=100):\\n    scores_deque = deque(maxlen=print_every)\\n    scores = []\\n    for i_episode in range(1, n_episodes+1):\\n        state = env.reset()\\n        agent.reset()\\n        score = 0\\n        for t in range(max_t):\\n            action = agent.act(state, add_noise=True)\\n            next_state, reward, done, _ = env.step(action)\\n            agent.step(state, action, reward, next_state, done)\\n            state = next_state\\n            score += reward\\n            if done:\\n                break \\n        scores_deque.append(score)\\n        scores.append(score)\\n        print(\\'\\rEpisode {}\\tAverage Score: {:.2f}\\'.format(i_episode, np.mean(scores_deque)), end=\"\")\\n        torch.save(agent.actor_local.state_dict(), \\'checkpoint_actor.pth\\')\\n        torch.save(agent.critic_local.state_dict(), \\'checkpoint_critic.pth\\')\\n        if i_episode % print_every == 0:\\n            print(\\'\\rEpisode {}\\tAverage Score: {:.2f}\\'.format(i_episode, np.mean(scores_deque)))\\n            \\n    return scores\\n\\nscores = ddpg()\\n\\nfig = plt.figure()\\nax = fig.add_subplot(111)\\nplt.plot(np.arange(1, len(scores)+1), scores)\\nplt.ylabel(\\'Score\\')\\nplt.xlabel(\\'Episode #\\')\\nplt.show()\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(2)\n",
    "agent = DDPG_agent(state_size=3, action_size=1, random_seed=2)\n",
    "\n",
    "def ddpg(n_episodes=1000, max_t=300, print_every=100):\n",
    "    scores_deque = deque(maxlen=print_every)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, add_noise=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)), end=\"\")\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Episode 100\tAverage Score: -1129.12\n",
    "Episode 200\tAverage Score: -924.719\n",
    "Episode 300\tAverage Score: -935.32\n",
    "Episode 400\tAverage Score: -824.84\n",
    "Episode 500\tAverage Score: -691.02\n",
    "Episode 600\tAverage Score: -501.29\n",
    "Episode 700\tAverage Score: -364.10\n",
    "Episode 800\tAverage Score: -382.40\n",
    "Episode 900\tAverage Score: -648.77\n",
    "Episode 1000\tAverage Score: -626.08\n",
    "\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-2        # learning rate of the critic\n",
    "Episode 100\tAverage Score: -1135.28\n",
    "Episode 200\tAverage Score: -812.514\n",
    "Episode 300\tAverage Score: -782.97\n",
    "\n",
    "Episode 100\tAverage Score: -1068.05\n",
    "Episode 200\tAverage Score: -653.161\n",
    "Episode 271\tAverage Score: -455.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "#install a few packages.  This line will take a few minutes to run!\n",
    "!pip -q install ./python\n",
    "#! pip install unityagents\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "#env = UnityEnvironment(file_name='Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')\n",
    "# get the default brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = DDPG_agent(state_size=state_size, action_size=action_size, num_agents=num_agents, random_seed=0)\n",
    "n_episodes = 1000\n",
    "print_every = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def ddpg(n_episodes=2000, max_t=1000):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations\n",
    "        agents.reset()\n",
    "        score = np.zeros(num_agents)\n",
    "        #print (state.shape)\n",
    "        for t in range(max_t):\n",
    "            action = agents.act(state)\n",
    "            #print (action)\n",
    "            #time.sleep(100)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            #print (state.shape)\n",
    "            #print (rewards.shape)\n",
    "            agents.step(state, action, rewards, next_state, dones)\n",
    "            state = next_state\n",
    "            score += rewards\n",
    "            if np.any(dones):\n",
    "                print('\\tSteps: ', t)\n",
    "                break \n",
    "        scores_deque.append(np.mean(score))\n",
    "        scores.append(np.mean(score))\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.3f}'.format(i_episode, \n",
    "                                                                          np.mean(scores_deque), \n",
    "                                                                          np.mean(score)))\n",
    "        average_score = np.mean(scores_deque)\n",
    "        if i_episode % print_every == 0 or average_score > 30:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            torch.save(agents.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agents.critic_local.state_dict(), 'checkpoint_critic.pth') \n",
    "            if average_score > 30:\n",
    "                break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1\tAverage Score: 0.56\tScore: 0.562\n",
      "Episode 2\tAverage Score: 0.61\tScore: 0.652\n",
      "Episode 3\tAverage Score: 0.54\tScore: 0.391\n",
      "Episode 4\tAverage Score: 0.61\tScore: 0.833\n",
      "Episode 5\tAverage Score: 0.66\tScore: 0.854\n",
      "Episode 6\tAverage Score: 0.76\tScore: 1.284\n",
      "Episode 7\tAverage Score: 0.81\tScore: 1.105\n",
      "Episode 8\tAverage Score: 0.86\tScore: 1.208\n",
      "Episode 9\tAverage Score: 0.90\tScore: 1.198\n",
      "Episode 10\tAverage Score: 0.96\tScore: 1.486\n",
      "Episode 10\tAverage Score: 0.96\n",
      "Episode 11\tAverage Score: 1.05\tScore: 2.026\n",
      "Episode 12\tAverage Score: 1.15\tScore: 2.186\n",
      "Episode 13\tAverage Score: 1.21\tScore: 1.952\n",
      "Episode 14\tAverage Score: 1.27\tScore: 1.995\n",
      "Episode 15\tAverage Score: 1.37\tScore: 2.741\n",
      "Episode 16\tAverage Score: 1.50\tScore: 3.446\n",
      "Episode 17\tAverage Score: 1.66\tScore: 4.324\n",
      "Episode 18\tAverage Score: 1.83\tScore: 4.729\n",
      "Episode 19\tAverage Score: 1.96\tScore: 4.244\n",
      "Episode 20\tAverage Score: 2.13\tScore: 5.448\n",
      "Episode 20\tAverage Score: 2.13\n",
      "Episode 21\tAverage Score: 2.23\tScore: 4.116\n",
      "Episode 22\tAverage Score: 2.39\tScore: 5.880\n",
      "Episode 23\tAverage Score: 2.60\tScore: 7.045\n",
      "Episode 24\tAverage Score: 2.74\tScore: 6.105\n",
      "Episode 25\tAverage Score: 3.04\tScore: 10.247\n",
      "Episode 26\tAverage Score: 3.31\tScore: 9.903\n",
      "Episode 27\tAverage Score: 3.60\tScore: 11.173\n",
      "Episode 28\tAverage Score: 3.89\tScore: 11.910\n",
      "Episode 29\tAverage Score: 4.27\tScore: 14.923\n",
      "Episode 30\tAverage Score: 4.59\tScore: 13.643\n",
      "Episode 30\tAverage Score: 4.59\n",
      "Episode 31\tAverage Score: 4.94\tScore: 15.532\n",
      "Episode 32\tAverage Score: 5.35\tScore: 17.905\n",
      "Episode 33\tAverage Score: 5.77\tScore: 19.325\n",
      "Episode 34\tAverage Score: 6.14\tScore: 18.298\n",
      "Episode 35\tAverage Score: 6.54\tScore: 20.056\n",
      "Episode 36\tAverage Score: 6.92\tScore: 20.352\n",
      "Episode 37\tAverage Score: 7.32\tScore: 21.875\n",
      "Episode 38\tAverage Score: 7.74\tScore: 23.323\n",
      "Episode 39\tAverage Score: 8.19\tScore: 25.119\n",
      "Episode 40\tAverage Score: 8.54\tScore: 22.298\n",
      "Episode 40\tAverage Score: 8.54\n",
      "Episode 41\tAverage Score: 8.88\tScore: 22.564\n",
      "Episode 42\tAverage Score: 9.25\tScore: 24.266\n",
      "Episode 43\tAverage Score: 9.63\tScore: 25.464\n",
      "Episode 44\tAverage Score: 10.04\tScore: 27.907\n",
      "Episode 45\tAverage Score: 10.49\tScore: 29.925\n",
      "Episode 46\tAverage Score: 10.92\tScore: 30.400\n",
      "Episode 47\tAverage Score: 11.31\tScore: 29.560\n",
      "Episode 48\tAverage Score: 11.76\tScore: 32.537\n",
      "Episode 49\tAverage Score: 12.19\tScore: 32.941\n",
      "Episode 50\tAverage Score: 12.63\tScore: 34.178\n",
      "Episode 50\tAverage Score: 12.63\n",
      "Episode 51\tAverage Score: 13.04\tScore: 33.347\n",
      "Episode 52\tAverage Score: 13.44\tScore: 34.037\n",
      "Episode 53\tAverage Score: 13.79\tScore: 32.228\n",
      "Episode 54\tAverage Score: 14.12\tScore: 31.193\n",
      "Episode 55\tAverage Score: 14.49\tScore: 34.656\n",
      "Episode 56\tAverage Score: 14.84\tScore: 34.405\n",
      "Episode 57\tAverage Score: 15.16\tScore: 32.793\n",
      "Episode 58\tAverage Score: 15.49\tScore: 34.445\n",
      "Episode 59\tAverage Score: 15.83\tScore: 35.646\n",
      "Episode 60\tAverage Score: 16.14\tScore: 34.349\n",
      "Episode 60\tAverage Score: 16.14\n",
      "Episode 61\tAverage Score: 16.46\tScore: 35.680\n",
      "Episode 62\tAverage Score: 16.76\tScore: 35.119\n",
      "Episode 63\tAverage Score: 17.02\tScore: 32.697\n",
      "Episode 64\tAverage Score: 17.30\tScore: 34.844\n",
      "Episode 65\tAverage Score: 17.56\tScore: 34.686\n",
      "Episode 66\tAverage Score: 17.80\tScore: 33.381\n",
      "Episode 67\tAverage Score: 18.05\tScore: 34.589\n",
      "Episode 68\tAverage Score: 18.31\tScore: 35.784\n",
      "Episode 69\tAverage Score: 18.51\tScore: 31.925\n",
      "Episode 70\tAverage Score: 18.74\tScore: 34.426\n",
      "Episode 70\tAverage Score: 18.74\n",
      "Episode 71\tAverage Score: 18.97\tScore: 34.866\n",
      "Episode 72\tAverage Score: 19.20\tScore: 35.939\n",
      "Episode 73\tAverage Score: 19.41\tScore: 34.711\n",
      "Episode 74\tAverage Score: 19.61\tScore: 33.888\n",
      "Episode 75\tAverage Score: 19.81\tScore: 34.661\n",
      "Episode 76\tAverage Score: 19.94\tScore: 29.329\n",
      "Episode 77\tAverage Score: 20.11\tScore: 33.442\n",
      "Episode 78\tAverage Score: 20.30\tScore: 34.881\n",
      "Episode 79\tAverage Score: 20.49\tScore: 35.484\n",
      "Episode 80\tAverage Score: 20.67\tScore: 34.474\n",
      "Episode 80\tAverage Score: 20.67\n",
      "Episode 81\tAverage Score: 20.82\tScore: 33.453\n",
      "Episode 82\tAverage Score: 20.99\tScore: 34.367\n",
      "Episode 83\tAverage Score: 21.14\tScore: 33.457\n",
      "Episode 84\tAverage Score: 21.29\tScore: 33.774\n",
      "Episode 85\tAverage Score: 21.43\tScore: 33.470\n",
      "Episode 86\tAverage Score: 21.59\tScore: 34.590\n",
      "Episode 87\tAverage Score: 21.74\tScore: 34.760\n",
      "Episode 88\tAverage Score: 21.90\tScore: 35.649\n",
      "Episode 89\tAverage Score: 22.06\tScore: 36.649\n",
      "Episode 90\tAverage Score: 22.16\tScore: 30.634\n",
      "Episode 90\tAverage Score: 22.16\n",
      "Episode 91\tAverage Score: 22.29\tScore: 34.606\n",
      "Episode 92\tAverage Score: 22.43\tScore: 34.759\n",
      "Episode 93\tAverage Score: 22.54\tScore: 33.011\n",
      "Episode 94\tAverage Score: 22.64\tScore: 31.692\n",
      "Episode 95\tAverage Score: 22.73\tScore: 30.712\n",
      "Episode 96\tAverage Score: 22.80\tScore: 29.711\n",
      "Episode 97\tAverage Score: 22.90\tScore: 32.452\n",
      "Episode 98\tAverage Score: 22.97\tScore: 30.108\n",
      "Episode 99\tAverage Score: 23.09\tScore: 34.406\n",
      "Episode 100\tAverage Score: 23.20\tScore: 34.521\n",
      "Episode 100\tAverage Score: 23.20\n",
      "Episode 101\tAverage Score: 23.52\tScore: 32.857\n",
      "Episode 102\tAverage Score: 23.88\tScore: 36.538\n",
      "Episode 103\tAverage Score: 24.24\tScore: 36.064\n",
      "Episode 104\tAverage Score: 24.59\tScore: 36.167\n",
      "Episode 105\tAverage Score: 24.94\tScore: 35.173\n",
      "Episode 106\tAverage Score: 25.25\tScore: 32.875\n",
      "Episode 107\tAverage Score: 25.57\tScore: 32.636\n",
      "Episode 108\tAverage Score: 25.90\tScore: 34.839\n",
      "Episode 109\tAverage Score: 26.24\tScore: 34.783\n",
      "Episode 110\tAverage Score: 26.57\tScore: 34.107\n",
      "Episode 110\tAverage Score: 26.57\n",
      "Episode 111\tAverage Score: 26.91\tScore: 36.759\n",
      "Episode 112\tAverage Score: 27.25\tScore: 36.116\n",
      "Episode 113\tAverage Score: 27.58\tScore: 34.472\n",
      "Episode 114\tAverage Score: 27.91\tScore: 34.991\n",
      "Episode 115\tAverage Score: 28.21\tScore: 33.135\n",
      "Episode 116\tAverage Score: 28.52\tScore: 34.132\n",
      "Episode 117\tAverage Score: 28.83\tScore: 35.822\n",
      "Episode 118\tAverage Score: 29.14\tScore: 35.659\n",
      "Episode 119\tAverage Score: 29.46\tScore: 36.076\n",
      "Episode 120\tAverage Score: 29.77\tScore: 36.101\n",
      "Episode 120\tAverage Score: 29.77\n",
      "Episode 121\tAverage Score: 30.08\tScore: 35.585\n",
      "Episode 121\tAverage Score: 30.08\n"
     ]
    }
   ],
   "source": [
    "score = ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
